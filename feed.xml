<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-10-31T01:03:42-04:00</updated><id>/feed.xml</id><title type="html">Steven Whitlow</title><subtitle>Steven Whitlow</subtitle><author><name>Steven Whitlow</name></author><entry><title type="html">Latent Dirichlet Allocation</title><link href="/blog/latent-dirichlet-allocation/" rel="alternate" type="text/html" title="Latent Dirichlet Allocation" /><published>2021-11-14T01:10:56-05:00</published><updated>2021-11-14T01:10:56-05:00</updated><id>/blog/latent-dirichlet-allocation</id><content type="html" xml:base="/blog/latent-dirichlet-allocation/"><![CDATA[<h2 id="text-as-data">Text as data</h2>
<p>Two primary issues:</p>
<ol>
  <li>Converting text data into something we can apply statistical methods to</li>
  <li>Dealing with the high dimensonality inherent in text data</li>
</ol>

<p>In the literature, a textual dataset is known as a <em>corpus</em>. Each corpus is composed of \(D\) <em>documents:</em> \(d \in {1,...,D}\). Each document is represented as collections from a vocabulary of \(W\) different <em>words:</em> \(w \in {1,...,W}\).</p>

<figure class="">
  <img src="/assets/img/document_diagram.png" alt="Summary of textual dataset" /><figcaption>
      Summary of textual dataset

    </figcaption></figure>

<p>For simplicity, assume each document \(d\) can be written as a sequence of \(N\) words: \(d = {w_{d,1},...,w_{d,N}}\). For most purposes, it is generally acceptable to ignore other elements of a document, such as punctuation, white space, etc. Even ignoring all non-word elements, there is a severe dimensonality problem: each document has \(W \times N\) possiblilities.</p>

<h3 id="processing-text">Processing text</h3>

<p>One way to reduce the complexity of text documents is to ignore the order of words, known as the <em>bag-of-words</em> model. Each document is represented as a vector of word counts for each word \(w\). The resulting \(D \times W\) matrix is called the <em>document-term matrix</em>. In some cases, we might be interested in combinations of words (eg. “labour market”). We can extend the bag-of-words representation to allow for n-grams (collections of \(n\) words).</p>

<p>To demonstrate this, let’s go through an example text dataset (reviews of electronics on Amazon) and go through the process of cleaning and developing the document-term matrix using the <code class="language-plaintext highlighter-rouge">nltk</code> package from Python.</p>

<p>Tokenization is the process of splitting a string into individual elements:</p>

<figure class="">
  <img src="/assets/img/text_2.png" alt="" /></figure>

<p>The distribution of words across any type of documents are typically well approximated by a power law distribution. This means the most common words, such as “the” or “and”, will account for a large fraction of the content of any corpus. These words (referred to as <strong>stop words</strong>) typically reveal little about the contents of a document, so it is common practice to drop them. I use the list of stop words provided by the <code class="language-plaintext highlighter-rouge">nltk</code> package.</p>

<p>For example, this specific review consisted of 59 words, but 32 were stop words:</p>
<figure class="">
  <img src="/assets/img/text_3.png" alt="" /></figure>

<p>In the context of an Amazon review, the words “impressed”, 
“impressing”, and “impress” likely convey the same information. One solution is to apply a <strong>stemmer</strong>, which reduces words to their stem (in this case, impress). The Porter stemmer algorithm proposed by Porter (1980) is one choice for this. Stemming leaves us with the following tokens for the example review:</p>

<figure class="">
  <img src="/assets/img/text_4.png" alt="" /></figure>

<p>The last step is to determine how to measure the intensity of each word in each document. The simplest way is to use the term frequency, that is, the raw count of how many times a word appears in the document: \(x_{d,w}\). The problem  with this approach is that for most corpora, some words will appear in most or all documents. One way to address this is to weight inverse document frequency:</p>

\[idf_{w} = \log\left(\frac{D}{\sum_{d=1}^{D}1(x_{d,w}&gt;0)}\right).\]

<p>which adjusts the term frequency to increase the weight of terms that appear relatively rarely throughout the corpus.</p>

<p>For the example review, the terms “ipad” and “work” both appear twice, but the resulting tf-idf is 9.243 and 3.992 respectively.</p>

<h2 id="latent-dirichlet-allocation">Latent Dirichlet allocation</h2>

<h3 id="motivation">Motivation</h3>

<p>In many cases, textual datasets can be very large. For instance, a newspaper article corpus might contain over one billion tokens and over one million unique tokens. Evidently, extreme dimensonality reduction is required. A solution is to keep only the top 250,000 words ranked by the sum of tf-idf, then go back to using the raw counts. At that point, one can use a topic model to extract relationships between documents. Latent Dirichlet allocation is one such topic model proposed by Blei, Jordan and Ng (JMLR, 2003) that allows for unsupervised classification of text data.</p>

<p>LDA takes as input the document-term matrix and returns two sets of distributions:</p>
<ol>
  <li>For each document d, a distribution over topics</li>
  <li>For each topic k, a distribution over words</li>
</ol>

<h3 id="baseline-naïve-bayes-mixture-of-unigrams">Baseline: Naïve Bayes (mixture of unigrams)</h3>
<p>The naive Bayes assumption, which can be applied in a topic modelling context by assumping that conditional on topic (class), the words within a document are independent.</p>

<p>Suppose each topic is assigned to a topic \(k \in \{1, \ldots K\}\) with probability \(\theta^{(k)}\), where \(\sum_{k}\theta^{(k)} = 1\). Topics are a mixture over words; i.e., for each word in a document of a given topic \(j\), the probability that a given word \(w\) is chosen is given by:</p>

\[P(w|k) = \phi_{k}^{(w)}, \quad \forall w \in\{1,\ldots,W\} \text{ and } k \in \{1, \ldots K\},\]

<p>where \(\sum_{w}\phi_{k}^{(w)} = 1 \; \forall \; k\in\{1, \ldots, K\}\).</p>

<p>The probability of each document \(d\):</p>

\[p(d) = \sum_{k}p(k) \prod_{n=1}^{N} (w_{n} | k) = \sum_{k} \theta^{(k)} \prod_{n=1}^{N} \phi_{k}^{(w_{n})}\]

<h4 id="how-do-we-solve-this-model">How do we solve this model?</h4>

<p>In the unsupervised case with unlabelled documents, we could use an expectation-maximization algorithm which iterates over:</p>

<ol>
  <li>Expectation step: write log-likelihood as an expectation over unobserved latent values given current parameter estimates</li>
  <li>Maximization step: run MLE over the model in (1)</li>
</ol>

<p>After estimating the parameters, it is easy to back out classifications using Bayes’ rule.</p>

<h3 id="lda-motivation">LDA: motivation</h3>

<p>Latent Dirichlet allocation improves the mixture of unigrams model along two dimensions.</p>

<ol>
  <li>
    <p>LDA takes as input the document-term matrix and returns two sets of distributions. Firstly, each document \(d\) is modelled as a distribution over topics. Each topic \(k\) is still defined as a distribution over words.  Rather than assign each document a unique topic, documents are modelled as multinomial mixtures over topics. The probability of a given word in document \(d\) being part of topic \(k\) is \(\theta_{d}^{(k)}\). Since topics remain mixtures over words, the probability of word \(w\) appearing given topic \(k\) is chosen is still \(\phi_{k}^{(w)}\).</p>
  </li>
  <li>
    <p>LDA places symmetric Dirichlet priors on the probability vectors for document-topic and topic-word shares. In particular, it assigns a \(K\)-dimensional Dirichlet prior with hyperparameter \(\alpha\) to each \(\theta_{d}\) and a \(W\)-dimensional Dirichlet prior with hyperparameter \(\eta\) to each \(\phi_{k}\). The Dirichlet distribution is a “distribution of distributions”: it is over vectors whose values are all in \([0,1]\) and sum to \(1\). This specifies the generative process for documents and smooths estimation.</p>
  </li>
</ol>

<p>We can write the generative process for LDA as follows:</p>

<ol>
  <li>Generate each topic \(k\), which is a distribution over words:</li>
</ol>

\[\phi_{k} \sim Dirichlet(\eta)\]

<ol>
  <li>For each document \(d\), generate a distribution over topics</li>
</ol>

\[\theta_{d} \sim Dirichlet(\alpha)\]

<ol>
  <li>
    <p>For each word \(i\) in document \(d\):</p>

    <p>3.1. Pick a topic for the word \(k_{d,i}\), drawing from \(Multinomial(\pmb{\theta_{d}})\).
 3.2. Pick a word from the topic \(w_{d,i}\), drawing from \(Multinomial(\pmb{\phi_{k}})\).</p>
  </li>
</ol>

<h3 id="lda-computation">LDA: Computation</h3>

<p>We can write the log likelihood as \(\sum_{d=1}^{D}\sum_{w=1}^{W}x_{d,w} \log (\sum_{k=1}^{K}\hat{\theta}_{d}^{(k)}\hat{\phi}_{k}^{(w)})\). The hyperparameters \(\alpha\) and \(\eta\) are often set to \(\alpha = \frac{50}{K}\) and \(\eta = \frac{200}{W}\), following Griffiths and Steyvers (PNAS, 2004). This leaves the selection of the value for \(K\); one can use the perplexity score, defined as \(\exp\left(- \frac{\mathcal{L}}{W} \right)\), a common measure in the NLP literature. However, in practice, this is often done on an ad-hoc basis.</p>

<p>Griffiths and Steyvers (2004) popularized the “smoothed” form of the LDA which can be estimated using Gibbs sampling. This is a MCMC approach used when the distribution to sample from is intractable, but sampling variables conditional on all others is not. When we have Dirichlet priors over both \(\theta\) and \(\phi\), we can use <em>collapsed</em> Gibbs sampling: since the priors are conjugate, they can be integrated out of the joint distribution. This means we only need to sample the topics \(k_{d,i}\).</p>

<!--
## Conclusion

You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:
<details>
  <summary>Code:</summary>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">ggplot</span><span class="p">(</span><span class="n">inner_join</span><span class="p">(</span><span class="n">sankey_data</span><span class="p">,</span><span class="n">mode_and_wage_changes</span><span class="p">),</span><span class="w">
       </span><span class="n">aes</span><span class="p">(</span><span class="n">axis1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode_former</span><span class="p">,</span><span class="w">
           </span><span class="n">axis2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode_current</span><span class="p">,</span><span class="w">
           </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">total</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_alluvium</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_wage</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_stratum</span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_text</span><span class="p">(</span><span class="n">stat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"stratum"</span><span class="p">,</span><span class="w"> 
            </span><span class="n">label.strata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_discrete</span><span class="p">(</span><span class="n">limits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Pre-displacement \n primary type"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Post-displacement \n primary type"</span><span class="p">),</span><span class="w">
                   </span><span class="n">expand</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-0.3</span><span class="p">,</span><span class="w"> </span><span class="m">0.3</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">guides</span><span class="p">(</span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">guide_legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">unname</span><span class="p">(</span><span class="n">TeX</span><span class="p">(</span><span class="s2">"$\\Delta w$"</span><span class="p">))))</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Share of total"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_minimal</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">scale_fill_fermenter</span><span class="p">(</span><span class="n">palette</span><span class="o">=</span><span class="s2">"YlGnBu"</span><span class="p">,</span><span class="w"> </span><span class="n">limits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-0.3</span><span class="p">,</span><span class="w"> </span><span class="m">0.3</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="c1">#scale_fill_viridis_b(limits = c(-0.3, 0.3)) +</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">16</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="o">=</span><span class="s2">"Palatino"</span><span class="p">),</span><span class="w"> </span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"bottom"</span><span class="p">)</span></code></pre></figure>

</details> <br />
Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->]]></content><author><name>Steven Whitlow</name></author><category term="blog" /><summary type="html"><![CDATA[This blog post provides a primer on the processing of text for document classification and introduces Naive Bayes and LDA as document classifcation methods.]]></summary></entry></feed>