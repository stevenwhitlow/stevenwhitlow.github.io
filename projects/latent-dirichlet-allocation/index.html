<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Latent Dirichlet Allocation - Steven Whitlow</title>
<meta name="description" content="This blog post provides a primer on the processing of text for document classification and introduces Naive Bayes and LDA as document classifcation methods.">


  <meta name="author" content="Steven Whitlow">
  
  <meta property="article:author" content="Steven Whitlow">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Steven Whitlow">
<meta property="og:title" content="Latent Dirichlet Allocation">
<meta property="og:url" content="/projects/latent-dirichlet-allocation/">


  <meta property="og:description" content="This blog post provides a primer on the processing of text for document classification and introduces Naive Bayes and LDA as document classifcation methods.">







  <meta property="article:published_time" content="2021-11-14T01:10:56-05:00">





  

  


<link rel="canonical" href="/projects/latent-dirichlet-allocation/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Steven Whitlow",
      "url": "/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Steven Whitlow Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->
<meta name='robots' content='noindex,nofollow' />
<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/img/title.png" alt=" "></a>
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/#about">About</a>
            </li><li class="masthead__menu-item">
              <a href="/#projects">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/#blog">Blog</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="/">
        <img src="/assets/img/avatar.png" alt="Steven Whitlow" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="/" itemprop="url">Steven Whitlow</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Quantitative social scientist</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Canada</span>
        </li>
      

      
        
          
            <li><a href="mailto:16sjw@queensu.ca" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email me</span></a></li>
          
        
          
            <li><a href="https://github.com/stevenwhitlow" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
        
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Latent Dirichlet Allocation">
    <meta itemprop="description" content="This blog post provides a primer on the processing of text for document classification and introduces Naive Bayes and LDA as document classifcation methods.">
    <meta itemprop="datePublished" content="2021-11-14T01:10:56-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="/projects/latent-dirichlet-allocation/" class="u-url" itemprop="url">Latent Dirichlet Allocation
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-keyboard"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#text-as-data">Text as data</a><ul><li><a href="#processing-text">Processing text</a></li></ul></li><li><a href="#latent-dirichlet-allocation">Latent Dirichlet allocation</a><ul><li><a href="#motivation">Motivation</a></li><li><a href="#baseline-naïve-bayes-mixture-of-unigrams">Baseline: Naïve Bayes (mixture of unigrams)</a><ul><li><a href="#how-do-we-solve-this-model">How do we solve this model?</a></li></ul></li><li><a href="#lda-motivation">LDA: motivation</a></li><li><a href="#lda-computation">LDA: Computation</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <h2 id="text-as-data">Text as data</h2>
<p>Two primary issues:</p>
<ol>
  <li>Converting text data into something we can apply statistical methods to</li>
  <li>Dealing with the high dimensonality inherent in text data</li>
</ol>

<p>In the literature, a textual dataset is known as a <em>corpus</em>. Each corpus is composed of \(D\) <em>documents:</em> \(d \in {1,...,D}\). Each document is represented as collections from a vocabulary of \(W\) different <em>words:</em> \(w \in {1,...,W}\).</p>

<figure class="">
  <img src="/assets/img/document_diagram.png" alt="Summary of textual dataset" /><figcaption>
      Summary of textual dataset

    </figcaption></figure>

<p>For simplicity, assume each document \(d\) can be written as a sequence of \(N\) words: \(d = {w_{d,1},...,w_{d,N}}\). For most purposes, it is generally acceptable to ignore other elements of a document, such as punctuation, white space, etc. Even ignoring all non-word elements, there is a severe dimensonality problem: each document has \(W \times N\) possiblilities.</p>

<h3 id="processing-text">Processing text</h3>

<p>One way to reduce the complexity of text documents is to ignore the order of words, known as the <em>bag-of-words</em> model. Each document is represented as a vector of word counts for each word \(w\). The resulting \(D \times W\) matrix is called the <em>document-term matrix</em>. In some cases, we might be interested in combinations of words (eg. “labour market”). We can extend the bag-of-words representation to allow for n-grams (collections of \(n\) words).</p>

<p>To demonstrate this, let’s go through an example text dataset (reviews of electronics on Amazon) and go through the process of cleaning and developing the document-term matrix using the <code class="language-plaintext highlighter-rouge">nltk</code> package from Python.</p>

<p>Tokenization is the process of splitting a string into individual elements:</p>

<figure class="">
  <img src="/assets/img/text_2.png" alt="" /></figure>

<p>The distribution of words across any type of documents are typically well approximated by a power law distribution. This means the most common words, such as “the” or “and”, will account for a large fraction of the content of any corpus. These words (referred to as <strong>stop words</strong>) typically reveal little about the contents of a document, so it is common practice to drop them. I use the list of stop words provided by the <code class="language-plaintext highlighter-rouge">nltk</code> package.</p>

<p>For example, this specific review consisted of 59 words, but 32 were stop words:</p>
<figure class="">
  <img src="/assets/img/text_3.png" alt="" /></figure>

<p>In the context of an Amazon review, the words “impressed”, 
“impressing”, and “impress” likely convey the same information. One solution is to apply a <strong>stemmer</strong>, which reduces words to their stem (in this case, impress). The Porter stemmer algorithm proposed by Porter (1980) is one choice for this. Stemming leaves us with the following tokens for the example review:</p>

<figure class="">
  <img src="/assets/img/text_4.png" alt="" /></figure>

<p>The last step is to determine how to measure the intensity of each word in each document. The simplest way is to use the term frequency, that is, the raw count of how many times a word appears in the document: \(x_{d,w}\). The problem  with this approach is that for most corpora, some words will appear in most or all documents. One way to address this is to weight  inverse document frequency:</p>

\[idf_{w} = \log\left(\frac{D}{\sum_{d=1}^{D}1(x_{d,w}&gt;0)}\right).\]

<p>which adjusts the term frequency
• For the example review, the terms “ipad” and “work” both appear twice, but the resulting tf-idf is 9.243 and 3.992 respectively</p>

<h2 id="latent-dirichlet-allocation">Latent Dirichlet allocation</h2>

<h3 id="motivation">Motivation</h3>

<p>The newspaper corpus used in this paper contains over one billion tokens and over one million unique tokens
• Evidently, extreme dimensonality reduction is required
• Keep only the top 250,000 words ranked by the sum of
tf-idf, then go back to using the raw counts
• Latent Dirichlet allocation is a topic model proposed by Blei, Jordan and Ng (JMLR, 2003)
• Unsupervised classification of text data
• LDA takes as input the document-term matrix and returns two sets of distributions:
1 For each document d, a distribution over topics
2 For each topic k, a distribution over words</p>

<h3 id="baseline-naïve-bayes-mixture-of-unigrams">Baseline: Naïve Bayes (mixture of unigrams)</h3>
<p>The naive Bayes assumption, which can be applied in a topic modelling context by assumping that conditional on topic (class), the words within a document are independent.</p>

<p>Suppose each topic is assigned to a topic \(k \in \{1, \ldots K\}\) with probability \(\theta^{(k)}\), where \(\sum_{k}\theta^{(k)} = 1\). Topics are a mixture over words; i.e., for each word in a document of a given topic \(j\), the probability that a given word \(w\) is chosen is given by:</p>

\[P(w|k) = \phi_{k}^{(w)}, \quad \forall w \in\{1,\ldots,W\} \text{ and } k \in \{1, \ldots K\},\]

<p>where \(\sum_{w}\phi_{k}^{(w)} = 1 \; \forall \; k\in\{1, \ldots, K\}\).</p>

<p>The probability of each document \(d\):</p>

\[p(d) = \sum_{k}p(k) \prod_{n=1}^{N} (w_{n} | k) = \sum_{k} \theta^{(k)} \prod_{n=1}^{N} \phi_{k}^{(w_{n})}\]

<h4 id="how-do-we-solve-this-model">How do we solve this model?</h4>

<p>In the unsupervised case with unlabelled documents, we could use an expectation-maximization algorithm which iterates over:</p>

<ol>
  <li>Expectation step: write log-likelihood as an expectation over unobserved latent values given current parameter estimates</li>
  <li>Maximization step: run MLE over the model in (1)</li>
</ol>

<p>After estimating the parameters, it is easy to back out classifications using Bayes’ rule.</p>

<h3 id="lda-motivation">LDA: motivation</h3>

<p>Latent Dirichlet allocation improves the mixture of unigrams model along two dimensions.</p>

<ol>
  <li>
    <p>LDA takes as input the document-term matrix and returns two sets of distributions. Firstly, each document \(d\) is modelled as a distribution over topics. Each topic \(k\) is still defined as a distribution over words.  Rather than assign each document a unique topic, documents are modelled as multinomial mixtures over topics. The probability of a given word in document \(d\) being part of topic \(k\) is \(\theta_{d}^{(k)}\). Since topics remain mixtures over words, the probability of word \(w\) appearing given topic \(k\) is chosen is still \(\phi_{k}^{(w)}\).</p>
  </li>
  <li>
    <p>LDA places symmetric Dirichlet priors on the probability vectors for document-topic and topic-word shares. In particular, it assigns a \(K\)-dimensional Dirichlet prior with hyperparameter \(\alpha\) to each \(\theta_{d}\) and a \(W\)-dimensional Dirichlet prior with hyperparameter \(\eta\) to each \(\phi_{k}\). The Dirichlet distribution is a “distribution of distributions”: it is over vectors whose values are all in \([0,1]\) and sum to \(1\). This specifies the generative process for documents and smooths estimation.</p>
  </li>
</ol>

<p>We can write the generative process for LDA as follows:</p>

<ol>
  <li>Generate each topic \(k\), which is a distribution over words:</li>
</ol>

\[\phi_{k} \sim Dirichlet(\eta)\]

<ol>
  <li>For each document \(d\), generate a distribution over topics</li>
</ol>

\[\theta_{d} \sim Dirichlet(\alpha)\]

<ol>
  <li>
    <p>For each word \(i\) in document \(d\):</p>

    <p>3.1. Pick a topic for the word \(k_{d,i}\), drawing from \(Multinomial(\pmb{\theta_{d}})\).
 3.2. Pick a word from the topic \(w_{d,i}\), drawing from \(Multinomial(\pmb{\phi_{k}})\).</p>
  </li>
</ol>

<h3 id="lda-computation">LDA: Computation</h3>

<p>We can write the log likelihood as \(\sum_{d=1}^{D}\sum_{w=1}^{W}x_{d,w} \log (\sum_{k=1}^{K}\hat{\theta}_{d}^{(k)}\hat{\phi}_{k}^{(w)})\). The hyperparameters \(\alpha\) and \(\eta\) are often set to \(\alpha = \frac{50}{K}\) and \(\eta = \frac{200}{W}\), following Griffiths and Steyvers (PNAS, 2004). This leaves the selection of the value for \(K\); one can use the perplexity score, defined as \(\exp\left(- \frac{\mathcal{L}}{W} \right)\), a common measure in the NLP literature. However, in practice, this is often done on an ad-hoc basis.</p>

<p>Griffiths and Steyvers (2004) popularized the “smoothed” form of the LDA which can be estimated using Gibbs sampling. This is a MCMC approach used when the distribution to sample from is intractable, but sampling variables conditional on all others is not. When we have Dirichlet priors over both \(\theta\) and \(\phi\), we can use <em>collapsed</em> Gibbs sampling: since the priors are conjugate, they can be integrated out of the joint distribution. This means we only need to sample the topics \(k_{d,i}\).</p>

<!--
## Conclusion

You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:
<details>
  <summary>Code:</summary>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">ggplot</span><span class="p">(</span><span class="n">inner_join</span><span class="p">(</span><span class="n">sankey_data</span><span class="p">,</span><span class="n">mode_and_wage_changes</span><span class="p">),</span><span class="w">
       </span><span class="n">aes</span><span class="p">(</span><span class="n">axis1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode_former</span><span class="p">,</span><span class="w">
           </span><span class="n">axis2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode_current</span><span class="p">,</span><span class="w">
           </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">total</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_alluvium</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_wage</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_stratum</span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_text</span><span class="p">(</span><span class="n">stat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"stratum"</span><span class="p">,</span><span class="w"> 
            </span><span class="n">label.strata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_discrete</span><span class="p">(</span><span class="n">limits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Pre-displacement \n primary type"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Post-displacement \n primary type"</span><span class="p">),</span><span class="w">
                   </span><span class="n">expand</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-0.3</span><span class="p">,</span><span class="w"> </span><span class="m">0.3</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">guides</span><span class="p">(</span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">guide_legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">unname</span><span class="p">(</span><span class="n">TeX</span><span class="p">(</span><span class="s2">"$\\Delta w$"</span><span class="p">))))</span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Share of total"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_minimal</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">scale_fill_fermenter</span><span class="p">(</span><span class="n">palette</span><span class="o">=</span><span class="s2">"YlGnBu"</span><span class="p">,</span><span class="w"> </span><span class="n">limits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-0.3</span><span class="p">,</span><span class="w"> </span><span class="m">0.3</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="c1">#scale_fill_viridis_b(limits = c(-0.3, 0.3)) +</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">16</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="o">=</span><span class="s2">"Palatino"</span><span class="p">),</span><span class="w"> </span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"bottom"</span><span class="p">)</span></code></pre></figure>

</details> <br />
Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#projects" class="page__taxonomy-item p-category" rel="tag">projects</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2021-11-14T01:10:56-05:00">November 14, 2021</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Latent+Dirichlet+Allocation%20%2Fprojects%2Flatent-dirichlet-allocation%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=%2Fprojects%2Flatent-dirichlet-allocation%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=%2Fprojects%2Flatent-dirichlet-allocation%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          
            
      </div>
    </div>
  
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Steven Whitlow. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

      


  <script>
  window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
  ga('create','UA-206400049-1','auto');
  ga('set', 'anonymizeIp', false);
  ga('send','pageview')
</script>
<script src="https://www.google-analytics.com/analytics.js" async></script>



   

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: ["tex2jax.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
       processEscapes: true
     },
     "HTML-CSS": {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    },
    TeX: { equationNumbers: { autoNumber: "AMS" } }
   });
</script>

  </body>
</html>
